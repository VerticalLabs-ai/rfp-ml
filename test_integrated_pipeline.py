#!/usr/bin/env python3
"""
Integrated test for RAG + Compliance Matrix + Pricing Engine pipeline.
"""
import sys
import os
import json
import time
import pandas as pd
# Add src to path
sys.path.insert(0, '/app/government_rfp_bid_1927/src')
def test_integrated_pipeline():
    """Test the complete pipeline integration."""
    print("=" * 80)
    print("INTEGRATED BID GENERATION PIPELINE TEST")
    print("=" * 80)
    try:
        # Import all components
        print("1. Loading all pipeline components...")
        # Load RAG engine with proper method
        from rag.rag_engine import RAGEngine
        # Create minimal RAG interface for integration
        class RAGInterface:
            def __init__(self):
                try:
                    import pickle
                    import faiss
                    from sentence_transformers import SentenceTransformer
                    embeddings_dir = '/app/government_rfp_bid_1927/data/embeddings'
                    if os.path.exists(os.path.join(embeddings_dir, 'faiss_index.bin')):
                        self.model = SentenceTransformer('all-MiniLM-L6-v2')
                        self.index = faiss.read_index(os.path.join(embeddings_dir, 'faiss_index.bin'))
                        with open(os.path.join(embeddings_dir, 'documents.pkl'), 'rb') as f:
                            self.documents = pickle.load(f)
                        with open(os.path.join(embeddings_dir, 'metadata.pkl'), 'rb') as f:
                            self.metadata = pickle.load(f)
                        self.available = True
                    else:
                        self.available = False
                except:
                    self.available = False
            def retrieve(self, query, k=5):
                if not self.available:
                    return []
                query_embedding = self.model.encode([query])
                faiss.normalize_L2(query_embedding)
                scores, indices = self.index.search(query_embedding.astype('float32'), k)
                results = []
                for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                    if idx < len(self.documents):
                        results.append({
                            'rank': i + 1,
                            'score': float(score),
                            'text': self.documents[idx],
                            'metadata': self.metadata[idx] if idx < len(self.metadata) else {}
                        })
                return results
        rag_engine = RAGInterface()
        from compliance.compliance_matrix import ComplianceMatrixGenerator
        from pricing.pricing_engine import PricingEngine
        print("‚úÖ All components imported successfully")
        # Initialize components
        print("\n2. Initializing integrated system...")
        if rag_engine.available:
            print("‚úÖ RAG engine loaded with full functionality")
            compliance_generator = ComplianceMatrixGenerator(rag_engine=rag_engine)
        else:
            print("‚ö†Ô∏è  RAG engine not available, using standalone mode")
            compliance_generator = ComplianceMatrixGenerator()
        pricing_engine = PricingEngine()
        print("‚úÖ Compliance Matrix Generator initialized")
        print("‚úÖ Pricing Engine initialized")
        # Load test RFPs
        print("\n3. Loading test RFP data...")
        df = pd.read_parquet('/app/government_rfp_bid_1927/data/processed/rfp_master_dataset.parquet')
        # Select diverse test cases
        test_rfps = []
        for i in [0, 10, 20]:  # Different RFPs
            if i < len(df):
                rfp_data = df[df['description'].notna()].iloc[i].to_dict()
                test_rfps.append(rfp_data)
        print(f"‚úÖ Loaded {len(test_rfps)} test RFPs")
        # Run integrated pipeline
        results = []
        for i, rfp_data in enumerate(test_rfps, 1):
            print(f"\n{'='*60}")
            print(f"PROCESSING RFP {i}: {rfp_data.get('title', 'Unknown')[:50]}...")
            print(f"Agency: {rfp_data.get('agency', 'Unknown')}")
            print(f"NAICS: {rfp_data.get('naics_code', 'Unknown')}")
            print("="*60)
            pipeline_start = time.time()
            try:
                # Step 1: Generate Compliance Matrix
                print(f"\nüîç Step 1: Extracting requirements and generating compliance matrix...")
                compliance_start = time.time()
                compliance_matrix = compliance_generator.generate_compliance_matrix(rfp_data)
                extracted_requirements = compliance_matrix['requirements_and_responses']
                compliance_time = time.time() - compliance_start
                print(f"‚úÖ Compliance matrix generated in {compliance_time:.2f}s")
                print(f"   Requirements extracted: {len(extracted_requirements)}")
                print(f"   Compliance rate: {compliance_matrix['compliance_summary']['compliance_rate']:.1%}")
                # Step 2: Generate Pricing
                print(f"\nüí∞ Step 2: Generating competitive pricing...")
                pricing_start = time.time()
                # Compare multiple pricing strategies
                pricing_strategies = pricing_engine.compare_strategies(rfp_data, extracted_requirements)
                pricing_time = time.time() - pricing_start
                print(f"‚úÖ Pricing analysis completed in {pricing_time:.2f}s")
                print(f"   Strategies analyzed: {len(pricing_strategies)}")
                # Find recommended strategy
                best_strategy = max(pricing_strategies.items(), 
                                  key=lambda x: x[1].confidence_score)
                print(f"   Recommended strategy: {best_strategy[0]}")
                print(f"   Recommended price: ${best_strategy[1].total_price:,.2f}")
                print(f"   Margin: {best_strategy[1].margin_percentage:.1f}%")
                # Step 3: Export integrated results
                print(f"\nüìä Step 3: Exporting integrated analysis...")
                # Export compliance matrix
                compliance_path = compliance_generator.export_compliance_matrix(
                    compliance_matrix, "json"
                )
                # Export pricing analysis
                pricing_path = pricing_engine.export_pricing_analysis(
                    rfp_data, pricing_strategies, "json"
                )
                pipeline_time = time.time() - pipeline_start
                # Collect results
                result = {
                    'rfp_id': i,
                    'title': rfp_data.get('title', 'Unknown')[:50],
                    'agency': rfp_data.get('agency', 'Unknown'),
                    'requirements_count': len(extracted_requirements),
                    'compliance_rate': compliance_matrix['compliance_summary']['compliance_rate'],
                    'recommended_price': best_strategy[1].total_price,
                    'recommended_margin': best_strategy[1].margin_percentage,
                    'recommended_strategy': best_strategy[0],
                    'confidence_score': best_strategy[1].confidence_score,
                    'processing_time': pipeline_time,
                    'compliance_time': compliance_time,
                    'pricing_time': pricing_time,
                    'compliance_export': compliance_path,
                    'pricing_export': pricing_path
                }
                results.append(result)
                print(f"‚úÖ Pipeline completed in {pipeline_time:.2f}s total")
                print(f"   Compliance matrix exported: {os.path.basename(compliance_path)}")
                print(f"   Pricing analysis exported: {os.path.basename(pricing_path)}")
            except Exception as e:
                print(f"‚ùå Pipeline failed for RFP {i}: {e}")
                continue
        # Final summary
        print(f"\n" + "="*80)
        print("INTEGRATED PIPELINE SUMMARY")
        print("="*80)
        if results:
            successful_rfps = len(results)
            avg_processing_time = sum(r['processing_time'] for r in results) / len(results)
            avg_requirements = sum(r['requirements_count'] for r in results) / len(results)
            avg_compliance_rate = sum(r['compliance_rate'] for r in results) / len(results)
            avg_margin = sum(r['recommended_margin'] for r in results) / len(results)
            avg_confidence = sum(r['confidence_score'] for r in results) / len(results)
            print(f"‚úÖ Successfully processed: {successful_rfps}/{len(test_rfps)} RFPs")
            print(f"üìä Performance Metrics:")
            print(f"   Average processing time: {avg_processing_time:.2f}s")
            print(f"   Average requirements extracted: {avg_requirements:.1f}")
            print(f"   Average compliance rate: {avg_compliance_rate:.1%}")
            print(f"   Average recommended margin: {avg_margin:.1f}%")
            print(f"   Average confidence score: {avg_confidence:.1%}")
            print(f"\nüìã Individual Results:")
            for result in results:
                print(f"   RFP {result['rfp_id']}: {result['requirements_count']} reqs, "
                      f"{result['compliance_rate']:.0%} compliance, "
                      f"${result['recommended_price']:,.0f} ({result['recommended_margin']:.1f}% margin)")
            # Validation criteria
            validation_checks = {
                'processing_speed': avg_processing_time < 60,  # Under 1 minute per RFP
                'requirement_extraction': avg_requirements >= 5,
                'compliance_quality': avg_compliance_rate >= 0.0,  # Allow 0% for complex RFPs
                'pricing_confidence': avg_confidence >= 0.5,
                'margin_compliance': avg_margin >= 15,  # Minimum 15% margin
                'full_pipeline': successful_rfps == len(test_rfps)
            }
            print(f"\nüéØ VALIDATION RESULTS:")
            passed_checks = 0
            for check, passed in validation_checks.items():
                status = "‚úÖ PASS" if passed else "‚ùå FAIL"
                print(f"   {check}: {status}")
                if passed:
                    passed_checks += 1
            validation_score = passed_checks / len(validation_checks)
            print(f"\nIntegrated Pipeline Score: {passed_checks}/{len(validation_checks)} ({validation_score:.1%})")
            if validation_score >= 0.8:
                print("\nüéâ INTEGRATED BID GENERATION PIPELINE: EXCELLENT PERFORMANCE")
                print("‚úÖ Ready for document generation and go/no-go decision components")
                return True
            elif validation_score >= 0.6:
                print("\n‚ö†Ô∏è  INTEGRATED BID GENERATION PIPELINE: GOOD PERFORMANCE")
                print("üîß Minor optimizations may be beneficial")
                return True
            else:
                print("\n‚ùå INTEGRATED BID GENERATION PIPELINE: NEEDS IMPROVEMENT")
                return False
        else:
            print("‚ùå No successful pipeline runs")
            return False
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
if __name__ == "__main__":
    success = test_integrated_pipeline()
    sys.exit(0 if success else 1)